{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.SQL and Dataframes\n",
    "\n",
    "References:\n",
    "\n",
    "* Spark-SQL, <https://spark.apache.org/docs/latest/sql-programming-guide.html#datasets-and-dataframes>\n",
    "\n",
    "\n",
    "# 5.1  Example Walkthrough\n",
    "Follow the Spark SQL and Dataframes Examples below!\n",
    "\n",
    "### Initialize PySpark\n",
    "\n",
    "First, we use the findspark package to initialize PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PySpark initiated... (cores: 16)\n"
     ]
    }
   ],
   "source": [
    "# Initialize PySpark\n",
    "APP_NAME = \"PySpark Lecture Herget\"\n",
    "SPARK_MASTER=\"spark://mpp3r01c03s03.cos.lrz.de:7077\"\n",
    "\n",
    "def initSC(cores):\n",
    "    # If there is no SparkSession, create the environment\n",
    "    try:\n",
    "        sc and spark\n",
    "    except NameError as e:\n",
    "      #import findspark\n",
    "      #findspark.init()\n",
    "        import pyspark\n",
    "        import pyspark.sql\n",
    "        from pyspark.sql import Row\n",
    "        conf=pyspark.SparkConf().setAppName(APP_NAME+\" \"+cores+\" CORES\").set(\"spark.cores.max\", cores)\n",
    "        sc = pyspark.SparkContext(master=SPARK_MASTER, conf=conf)\n",
    "        spark = pyspark.sql.SparkSession(sc).builder.appName(APP_NAME+\" \"+cores+\" CORES\").getOrCreate()\n",
    "    print(\"PySpark initiated... (cores: \"+cores+\")\")\n",
    "    return sc, spark\n",
    "\n",
    "def reinitSC(cores, sc):\n",
    "    import pyspark\n",
    "    import pyspark.sql    \n",
    "    sc.stop()\n",
    "    \n",
    "    # make config changes and restart\n",
    "    conf=pyspark.SparkConf().setAppName(APP_NAME+\" \"+cores+\" CORES\").set(\"spark.cores.max\", cores)\n",
    "    sc = pyspark.SparkContext(master=SPARK_MASTER, conf=conf)\n",
    "    #spark = pyspark.sql.SparkSession(sc).builder \\\n",
    "     #   .appName(APP_NAME+\" \"+cores+\" CORES\") \\\n",
    "      #  .config('spark.cores.max', cores) \\\n",
    "       # .getOrCreate() \n",
    "    spark = pyspark.sql.SparkSession(sc).builder.appName(APP_NAME+\" \"+cores+\" CORES\").getOrCreate()\n",
    "    #sc = spark.sparkContext    \n",
    "    print(\"PySpark reinitiated... (cores: \"+cores+\")\")\n",
    "    return sc, spark\n",
    "\n",
    "sc, spark = initSC('16')\n",
    "\n",
    "from pyspark.sql import Row\n",
    "def nasa_split_http(line):\n",
    "    parts = line.split(' ')\n",
    "    if (len(parts) > 2):\n",
    "        row = Row(\n",
    "            ip=parts[0],\n",
    "            http_response=parts[-2],\n",
    "        )\n",
    "        return row\n",
    "    else:\n",
    "        return {}\n",
    "\n",
    "def nasafileprep():\n",
    "    nasa = sc.textFile(\"../data/nasa/NASA_access_log_Jul95\")\n",
    "    nasa_rows = nasa.flatMap(lambda line: line.split('\\n')).map(nasa_split_http)\n",
    "    # Convert to a pyspark.sql.DataFrame\n",
    "    nasa_rows_df = nasa_rows.toDF()\n",
    "\n",
    "    # Register the DataFrame for Spark SQL\n",
    "    nasa_rows_df.registerTempTable(\"ip_response\")\n",
    "    return nasa_rows_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hello, World!\n",
    "\n",
    "Loading data, mapping it and collecting the records into RAM..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PySpark reinitiated... (cores: 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['Russell Jurney', 'Relato', 'CEO'],\n",
       " ['Florian Liebert', 'Mesosphere', 'CEO'],\n",
       " ['Don Brown', 'Rocana', 'CIO'],\n",
       " ['Steve Jobs', 'Apple', 'CEO'],\n",
       " ['Donald Trump', 'The Trump Organization', 'CEO'],\n",
       " ['Russell Jurney', 'Data Syndrome', 'Principal Consultant']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the text file using the SparkContext\n",
    "csv_lines = sc.textFile(\"../data/example.csv\")\n",
    "\n",
    "# Map the data to split the lines into a list\n",
    "data = csv_lines.map(lambda line: line.split(\",\"))\n",
    "\n",
    "# Collect the dataset into local RAM\n",
    "data.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Rows\n",
    "\n",
    "Creating `pyspark.sql.Rows` out of your data so you can create DataFrames..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the CSV into a pyspark.sql.Row\n",
    "def csv_to_row(line):\n",
    "    parts = line.split(\",\")\n",
    "    row = Row(\n",
    "      name=parts[0],\n",
    "      company=parts[1],\n",
    "      title=parts[2]\n",
    "    )\n",
    "    return row\n",
    "\n",
    "# Apply the function to get rows in an RDD\n",
    "rows = csv_lines.map(csv_to_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating DataFrames from RDDs\n",
    "\n",
    "Using the `RDD.toDF()` method to create a dataframe, registering the `DataFrame` as a temporary table with Spark SQL, and counting the jobs per person using Spark SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|           name|total|\n",
      "+---------------+-----+\n",
      "|   Donald Trump|    1|\n",
      "|Florian Liebert|    1|\n",
      "|      Don Brown|    1|\n",
      "| Russell Jurney|    2|\n",
      "|     Steve Jobs|    1|\n",
      "+---------------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(name='Donald Trump', total=1),\n",
       " Row(name='Florian Liebert', total=1),\n",
       " Row(name='Don Brown', total=1),\n",
       " Row(name='Russell Jurney', total=2),\n",
       " Row(name='Steve Jobs', total=1)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert to a pyspark.sql.DataFrame\n",
    "rows_df = rows.toDF()\n",
    "\n",
    "# Register the DataFrame for Spark SQL\n",
    "rows_df.registerTempTable(\"executives\")\n",
    "\n",
    "# Generate a new DataFrame with SQL using the SparkSession\n",
    "job_counts = spark.sql(\"\"\"\n",
    "SELECT\n",
    "  name,\n",
    "  COUNT(*) AS total\n",
    "  FROM executives\n",
    "  GROUP BY name\n",
    "\"\"\")\n",
    "job_counts.show()\n",
    "\n",
    "# Go back to an RDD\n",
    "job_counts.rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.2-5.9 NASA DataSet\n",
    "\n",
    "5.2 Create a Spark-SQL table with fields for IP/Host and Response Code from the NASA Log file! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#199.72.81.55 - - [01/Jul/1995:00:00:01 -0400] \"GET /history/apollo/ HTTP/1.0\" 200 6245\n",
    "#row = Row(\n",
    "#    ip=parts[0],\n",
    "#    date=parts[3]+' '+parts[4],\n",
    "#    http_method=parts[5].replace('\\\"', ''),\n",
    "#    http_url=parts[6],\n",
    "#    http_version=parts[7].replace('\"',''),\n",
    "#    http_response=parts[8],\n",
    "#    response_time=parts[9]\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+\n",
      "|http_response|                  ip|\n",
      "+-------------+--------------------+\n",
      "|          200|        199.72.81.55|\n",
      "|          200|unicomp6.unicomp.net|\n",
      "|          200|      199.120.110.21|\n",
      "|          304|  burger.letters.com|\n",
      "|          200|      199.120.110.21|\n",
      "|          304|  burger.letters.com|\n",
      "|          200|  burger.letters.com|\n",
      "|          200|     205.212.115.106|\n",
      "|          200|         d104.aa.net|\n",
      "|          200|      129.94.144.152|\n",
      "|          200|unicomp6.unicomp.net|\n",
      "|          200|unicomp6.unicomp.net|\n",
      "|          200|unicomp6.unicomp.net|\n",
      "|          200|         d104.aa.net|\n",
      "|          200|         d104.aa.net|\n",
      "|          200|         d104.aa.net|\n",
      "|          304|      129.94.144.152|\n",
      "|          200|      199.120.110.21|\n",
      "|          200|ppptky391.asahi-n...|\n",
      "|          200|  net-1-141.eden.com|\n",
      "+-------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nasa_rows_df = nasafileprep()\n",
    "# Generate a new DataFrame with SQL using the SparkSession\n",
    "job_show = spark.sql(\"\"\"\n",
    "SELECT\n",
    "  *\n",
    "  FROM ip_response\n",
    "\"\"\")\n",
    "job_show.show()\n",
    "\n",
    "# Go back to an RDD\n",
    "#job_show.rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.3 Run an SQL query that outputs the number of occurrences of each HTTP response code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+\n",
      "|http_response|count(http_response)|\n",
      "+-------------+--------------------+\n",
      "|          200|             1701534|\n",
      "|         null|                   0|\n",
      "|          302|               46573|\n",
      "|          501|                  14|\n",
      "|          404|               10845|\n",
      "|          403|                  54|\n",
      "|          500|                  62|\n",
      "|          304|              132627|\n",
      "|          400|                   5|\n",
      "+-------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(http_response='200', count(http_response)=1701534),\n",
       " Row(http_response=None, count(http_response)=0),\n",
       " Row(http_response='302', count(http_response)=46573),\n",
       " Row(http_response='501', count(http_response)=14),\n",
       " Row(http_response='404', count(http_response)=10845),\n",
       " Row(http_response='403', count(http_response)=54),\n",
       " Row(http_response='500', count(http_response)=62),\n",
       " Row(http_response='304', count(http_response)=132627),\n",
       " Row(http_response='400', count(http_response)=5)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate a new DataFrame with SQL using the SparkSession\n",
    "job_show_53 = spark.sql(\"\"\"\n",
    "SELECT\n",
    "  http_response, count(http_response)\n",
    "  FROM ip_response\n",
    "  GROUP BY http_response\n",
    "\"\"\")\n",
    "job_show_53.show()\n",
    " \n",
    "# Go back to an RDD\n",
    "job_show_53.rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.4 Cachen Sie den Dataframe und führen Sie dieselbe Query nochmals aus! Messen Sie die Laufzeit für das Cachen und für die Ausführungszeit der Query!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache time:  0.01969146728515625\n",
      "+-------------+--------------------+\n",
      "|http_response|count(http_response)|\n",
      "+-------------+--------------------+\n",
      "|          200|             1701534|\n",
      "|         null|                   0|\n",
      "|          302|               46573|\n",
      "|          501|                  14|\n",
      "|          404|               10845|\n",
      "|          403|                  54|\n",
      "|          500|                  62|\n",
      "|          304|              132627|\n",
      "|          400|                   5|\n",
      "+-------------+--------------------+\n",
      "\n",
      "Query time (uncached):  43.43684935569763\n",
      "+-------------+--------------------+\n",
      "|http_response|count(http_response)|\n",
      "+-------------+--------------------+\n",
      "|          200|             1701534|\n",
      "|         null|                   0|\n",
      "|          302|               46573|\n",
      "|          501|                  14|\n",
      "|          404|               10845|\n",
      "|          403|                  54|\n",
      "|          500|                  62|\n",
      "|          304|              132627|\n",
      "|          400|                   5|\n",
      "+-------------+--------------------+\n",
      "\n",
      "Query time cached:  4.207841157913208\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[http_response: string, ip: string]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "nasa_rows_df.cache()\n",
    "\n",
    "start = time.time()\n",
    "nasa_rows_df.cache()\n",
    "end = time.time()\n",
    "print(\"Cache time: \",end - start)\n",
    "\n",
    "start = time.time()\n",
    "job_show_53 = spark.sql(\"\"\"\n",
    "SELECT\n",
    "  http_response, count(http_response)\n",
    "  FROM ip_response\n",
    "  GROUP BY http_response\n",
    "\"\"\")\n",
    "job_show_53.show()\n",
    "end = time.time()\n",
    "print(\"Query time (uncached): \",end - start)\n",
    "\n",
    "start = time.time()\n",
    "job_show_53 = spark.sql(\"\"\"\n",
    "SELECT\n",
    "  http_response, count(http_response)\n",
    "  FROM ip_response\n",
    "  GROUP BY http_response\n",
    "\"\"\")\n",
    "job_show_53.show()\n",
    "end = time.time()\n",
    "print(\"Query time cached: \",end - start)\n",
    "nasa_rows_df.cache().unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.5. Implement the same Query using the Dataframe API!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.6 Führen Sie diesselbe Query mit/ohne Cache und 8, 16 Cores aus! Dokumentieren und erklären Sie das Ergebnis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PySpark reinitiated... (cores: 1)\n",
      "[('spark.cores.max', '1'), ('spark.app.id', 'app-20180406134454-0101'), ('spark.master', 'spark://mpp3r01c03s03.cos.lrz.de:7077'), ('spark.app.name', 'PySpark Lecture Herget 1 CORES'), ('spark.rdd.compress', 'True'), ('spark.serializer.objectStreamReset', '100'), ('spark.executor.id', 'driver'), ('spark.submit.deployMode', 'client'), ('spark.driver.port', '38446'), ('spark.ui.showConsoleProgress', 'true'), ('spark.driver.host', 'mpp3r01c03s02-op')]\n",
      "   Init cache\n",
      "+-------------+--------------------+\n",
      "|http_response|count(http_response)|\n",
      "+-------------+--------------------+\n",
      "|          200|             1701534|\n",
      "|         null|                   0|\n",
      "|          302|               46573|\n",
      "|          501|                  14|\n",
      "|          404|               10845|\n",
      "|          403|                  54|\n",
      "|          500|                  62|\n",
      "|          304|              132627|\n",
      "|          400|                   5|\n",
      "+-------------+--------------------+\n",
      "\n",
      "      Query time (uncached):  151.09838199615479\n",
      "    Starting benchmark\n",
      "+-------------+--------------------+\n",
      "|http_response|count(http_response)|\n",
      "+-------------+--------------------+\n",
      "|          200|             1701534|\n",
      "|         null|                   0|\n",
      "|          302|               46573|\n",
      "|          501|                  14|\n",
      "|          404|               10845|\n",
      "|          403|                  54|\n",
      "|          500|                  62|\n",
      "|          304|              132627|\n",
      "|          400|                   5|\n",
      "+-------------+--------------------+\n",
      "\n",
      "      Query time (cores: 1, cached: True):  23.264286756515503\n",
      "    Starting benchmark\n",
      "+-------------+--------------------+\n",
      "|http_response|count(http_response)|\n",
      "+-------------+--------------------+\n",
      "|          200|             1701534|\n",
      "|         null|                   0|\n",
      "|          302|               46573|\n",
      "|          501|                  14|\n",
      "|          404|               10845|\n",
      "|          403|                  54|\n",
      "|          500|                  62|\n",
      "|          304|              132627|\n",
      "|          400|                   5|\n",
      "+-------------+--------------------+\n",
      "\n",
      "      Query time (cores: 1, cached: False):  147.1105170249939\n",
      "[{'cores': 1, 'cached': True, 'time': 23.264286756515503}, {'cores': 1, 'cached': False, 'time': 147.1105170249939}]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "cors = [1]\n",
    "cachs = [True, False]\n",
    "result = []\n",
    "for cor in cors:\n",
    "    # Init Cores\n",
    "    sc, spark = reinitSC(str(cor), sc)\n",
    "    print(sc._conf.getAll())\n",
    "    for cach in cachs:\n",
    "        nasa_rows_df = nasafileprep()\n",
    "        if cach:\n",
    "            # Init Cache\n",
    "            print(\"   Init cache\")\n",
    "            nasa_rows_df.cache()\n",
    "            start = time.time()\n",
    "            job_show_53 = spark.sql(\"\"\"\n",
    "            SELECT\n",
    "              http_response, count(http_response)\n",
    "              FROM ip_response\n",
    "              GROUP BY http_response\n",
    "            \"\"\")\n",
    "            job_show_53.show()\n",
    "            end = time.time()\n",
    "            print(\"      Query time (uncached): \",end - start)\n",
    "        # Do benchmark\n",
    "        print(\"    Starting benchmark\")\n",
    "        bstart = time.time()\n",
    "        job_show_53 = spark.sql(\"\"\"\n",
    "        SELECT\n",
    "          http_response, count(http_response)\n",
    "          FROM ip_response\n",
    "          GROUP BY http_response\n",
    "        \"\"\")\n",
    "        job_show_53.show()\n",
    "        bend = time.time()\n",
    "        print(\"      Query time (cores: \"+str(cor)+\", cached: \"+str(cach)+\"): \",bend - bstart)\n",
    "        result.append({'cores': cor, 'cached': cach, 'time': bend - bstart})\n",
    "        if cach:\n",
    "            nasa_rows_df.cache().unpersist()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result:\n",
    "% [{'cores': 16, 'cached': True, 'time': 2.846684455871582}, {'cores': 16, 'cached': False, 'time': 28.683916091918945}, {'cores': 8, 'cached': True, 'time': 4.1324403285980225}, {'cores': 8, 'cached': False, 'time': 24.340951204299927}]\n",
    "[{'cores': 1, 'cached': True, 'time': 23.264286756515503}, {'cores': 1, 'cached': False, 'time': 147.1105170249939}]\n",
    "\n",
    "| Cores | Cached | Measured time|\n",
    "|---|---|---|\n",
    "| 1     | no     | 147.1105 |\n",
    "| 1     | yes    | 23.2642|\n",
    "| 8     | no     | 24.3409|\n",
    "| 8     | yes    | 4.1324|\n",
    "| 16    | no     | 28.6839|\n",
    "| 16    | yes    | 2.8466 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.7 Performance Analysis: \n",
    "* Create RDDs with 2x, 4x, 8x and 16x of the size of the NASA log dataset! Persist the dataset in the Spark Cache! Use an appropriate number of cores (e.g. 8 or 16)!\n",
    "* Measure and plot the response times for all datasets using a constant number of cores!\n",
    "* Plot the results!\n",
    "* Explain the results!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "'requirement failed: Can only call getServletHandlers on a running MetricsSystem'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/naslx/projects/pn69si/mnmda001/software/anaconda3/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/naslx/projects/pn69si/mnmda001/software/anaconda3/lib/python3.6/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    319\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    321\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.lang.IllegalArgumentException: requirement failed: Can only call getServletHandlers on a running MetricsSystem\n\tat scala.Predef$.require(Predef.scala:224)\n\tat org.apache.spark.metrics.MetricsSystem.getServletHandlers(MetricsSystem.scala:91)\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:515)\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:238)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-4871c63c3378>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreinitSC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'8'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msiz\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-ca17fa7f37f8>\u001b[0m in \u001b[0;36mreinitSC\u001b[0;34m(cores, sc)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m# make config changes and restart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetAppName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAPP_NAME\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcores\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\" CORES\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spark.cores.max\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaster\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSPARK_MASTER\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0;31m#spark = pyspark.sql.SparkSession(sc).builder \\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m      \u001b[0;31m#   .appName(APP_NAME+\" \"+cores+\" CORES\") \\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/naslx/projects/pn69si/mnmda001/software/anaconda3/lib/python3.6/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n\u001b[0;32m--> 118\u001b[0;31m                           conf, jsc, profiler_cls)\n\u001b[0m\u001b[1;32m    119\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# If an error occurs, clean up in order to allow future SparkContext creation:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/naslx/projects/pn69si/mnmda001/software/anaconda3/lib/python3.6/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_do_init\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0;31m# Create the Java SparkContext through Py4J\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjsc\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m         \u001b[0;31m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_jconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/naslx/projects/pn69si/mnmda001/software/anaconda3/lib/python3.6/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_initialize_context\u001b[0;34m(self, jconf)\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0mInitialize\u001b[0m \u001b[0mSparkContext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0mto\u001b[0m \u001b[0mallow\u001b[0m \u001b[0msubclass\u001b[0m \u001b[0mspecific\u001b[0m \u001b[0minitialization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \"\"\"\n\u001b[0;32m--> 270\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJavaSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/naslx/projects/pn69si/mnmda001/software/anaconda3/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1426\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1427\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1428\u001b[0;31m             answer, self._gateway_client, None, self._fqn)\n\u001b[0m\u001b[1;32m   1429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1430\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/naslx/projects/pn69si/mnmda001/software/anaconda3/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mQueryExecutionException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'java.lang.IllegalArgumentException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mIllegalArgumentException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: 'requirement failed: Can only call getServletHandlers on a running MetricsSystem'"
     ]
    }
   ],
   "source": [
    "import time\n",
    "result = []\n",
    "sc, spark = reinitSC('8', sc)\n",
    "size = [1, 2, 4, 8, 16]\n",
    "for siz in size: \n",
    "    print(\"Prepare file for size: \",siz)\n",
    "    nasa = sc.textFile(\"../data/nasa/NASA_access_log_Jul95\")\n",
    "    for i in range(0, siz-1): \n",
    "        nasa_un = nasa.union(nasa)\n",
    "    if siz == 1:\n",
    "        nasa_un = nasa\n",
    "    nasa_rows = nasa_un.flatMap(lambda line: line.split('\\n')).map(nasa_split_http)\n",
    "    nasa_rows_df = nasa_rows.toDF()\n",
    "    nasa_rows_df.registerTempTable(\"ip_response\")\n",
    "    \n",
    "    print(\"   Init cache\")\n",
    "    nasa_rows_df.cache()\n",
    "    start = time.time()\n",
    "    job_show_53 = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "      http_response, count(http_response)\n",
    "      FROM ip_response\n",
    "      GROUP BY http_response\n",
    "    \"\"\")\n",
    "    job_show_53.show()\n",
    "    end = time.time()\n",
    "    print(\"      Query time (uncached): \",end - start)\n",
    "    \n",
    "    print(\"    Starting benchmark\")\n",
    "    bstart = time.time()\n",
    "    job_show_53 = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "      http_response, count(http_response)\n",
    "      FROM ip_response\n",
    "      GROUP BY http_response\n",
    "    \"\"\")\n",
    "    job_show_53.show()\n",
    "    bend = time.time()\n",
    "    print(\"      Query time (cores: 16, size: \"+str(siz)+\"): \",bend - bstart)\n",
    "    result.append({'cores': 16, 'size': siz, 'time': bend - bstart})\n",
    "\n",
    "pyplot_result_57 = result\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "x=[]\n",
    "y=[]\n",
    "for res in pyplot_result_57:\n",
    "    y.append(res['time'])\n",
    "    x.append(res['size'])\n",
    "plt.plot(x,y)\n",
    "plt.xlabel('size')\n",
    "plt.ylabel('time')\n",
    "plt.title('16 Cores')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.8 Strong Scaling\n",
    "\n",
    "  * Measure the runtime for the query for 8, 16, 32, 64, 128, 256 cores for 1x and 16x datasets! Datasets cached in Memory!\n",
    "  * Compute the speedup and efficiency!\n",
    "  * Plot the responses!\n",
    "  * Explain the results!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.9 Convert the output to a Pandas dataframe and calculate the percentage of total for each response code!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "do not need\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
