{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.SQL and Dataframes\n",
    "\n",
    "References:\n",
    "\n",
    "* Spark-SQL, <https://spark.apache.org/docs/latest/sql-programming-guide.html#datasets-and-dataframes>\n",
    "\n",
    "\n",
    "# 5.1  Example Walkthrough\n",
    "Follow the Spark SQL and Dataframes Examples below!\n",
    "\n",
    "### Initialize PySpark\n",
    "\n",
    "First, we use the findspark package to initialize PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PySpark initiated...\n"
     ]
    }
   ],
   "source": [
    "# Initialize PySpark\n",
    "APP_NAME = \"PySpark Lecture Herget\"\n",
    "SPARK_MASTER=\"spark://mpp3r01c03s03.cos.lrz.de:7077\"\n",
    "\n",
    "# If there is no SparkSession, create the environment\n",
    "try:\n",
    "    sc and spark\n",
    "except NameError as e:\n",
    "  #import findspark\n",
    "  #findspark.init()\n",
    "    import pyspark\n",
    "    import pyspark.sql\n",
    "    from pyspark.sql import Row\n",
    "    conf=pyspark.SparkConf().setAppName(APP_NAME).set(\"spark.cores.max\", \"8\")\n",
    "    sc = pyspark.SparkContext(master=SPARK_MASTER, conf=conf)\n",
    "    spark = pyspark.sql.SparkSession(sc).builder.appName(APP_NAME).getOrCreate()\n",
    "\n",
    "print(\"PySpark initiated...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hello, World!\n",
    "\n",
    "Loading data, mapping it and collecting the records into RAM..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Russell Jurney', 'Relato', 'CEO'],\n",
       " ['Florian Liebert', 'Mesosphere', 'CEO'],\n",
       " ['Don Brown', 'Rocana', 'CIO'],\n",
       " ['Steve Jobs', 'Apple', 'CEO'],\n",
       " ['Donald Trump', 'The Trump Organization', 'CEO'],\n",
       " ['Russell Jurney', 'Data Syndrome', 'Principal Consultant']]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the text file using the SparkContext\n",
    "csv_lines = sc.textFile(\"../data/example.csv\")\n",
    "\n",
    "# Map the data to split the lines into a list\n",
    "data = csv_lines.map(lambda line: line.split(\",\"))\n",
    "\n",
    "# Collect the dataset into local RAM\n",
    "data.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Rows\n",
    "\n",
    "Creating `pyspark.sql.Rows` out of your data so you can create DataFrames..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the CSV into a pyspark.sql.Row\n",
    "def csv_to_row(line):\n",
    "    parts = line.split(\",\")\n",
    "    row = Row(\n",
    "      name=parts[0],\n",
    "      company=parts[1],\n",
    "      title=parts[2]\n",
    "    )\n",
    "    return row\n",
    "\n",
    "# Apply the function to get rows in an RDD\n",
    "rows = csv_lines.map(csv_to_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating DataFrames from RDDs\n",
    "\n",
    "Using the `RDD.toDF()` method to create a dataframe, registering the `DataFrame` as a temporary table with Spark SQL, and counting the jobs per person using Spark SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|           name|total|\n",
      "+---------------+-----+\n",
      "|   Donald Trump|    1|\n",
      "|Florian Liebert|    1|\n",
      "|      Don Brown|    1|\n",
      "| Russell Jurney|    2|\n",
      "|     Steve Jobs|    1|\n",
      "+---------------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(name='Donald Trump', total=1),\n",
       " Row(name='Florian Liebert', total=1),\n",
       " Row(name='Don Brown', total=1),\n",
       " Row(name='Russell Jurney', total=2),\n",
       " Row(name='Steve Jobs', total=1)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert to a pyspark.sql.DataFrame\n",
    "rows_df = rows.toDF()\n",
    "\n",
    "# Register the DataFrame for Spark SQL\n",
    "rows_df.registerTempTable(\"executives\")\n",
    "\n",
    "# Generate a new DataFrame with SQL using the SparkSession\n",
    "job_counts = spark.sql(\"\"\"\n",
    "SELECT\n",
    "  name,\n",
    "  COUNT(*) AS total\n",
    "  FROM executives\n",
    "  GROUP BY name\n",
    "\"\"\")\n",
    "job_counts.show()\n",
    "\n",
    "# Go back to an RDD\n",
    "job_counts.rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.2-5.9 NASA DataSet\n",
    "\n",
    "5.2 Create a Spark-SQL table with fields for IP/Host and Response Code from the NASA Log file! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+\n",
      "|http_response|                  ip|\n",
      "+-------------+--------------------+\n",
      "|          200|        199.72.81.55|\n",
      "|          200|unicomp6.unicomp.net|\n",
      "|          200|      199.120.110.21|\n",
      "|          304|  burger.letters.com|\n",
      "|          200|      199.120.110.21|\n",
      "|          304|  burger.letters.com|\n",
      "|          200|  burger.letters.com|\n",
      "|          200|     205.212.115.106|\n",
      "|          200|         d104.aa.net|\n",
      "|          200|      129.94.144.152|\n",
      "|          200|unicomp6.unicomp.net|\n",
      "|          200|unicomp6.unicomp.net|\n",
      "|          200|unicomp6.unicomp.net|\n",
      "|          200|         d104.aa.net|\n",
      "|          200|         d104.aa.net|\n",
      "|          200|         d104.aa.net|\n",
      "|          304|      129.94.144.152|\n",
      "|          200|      199.120.110.21|\n",
      "|          200|ppptky391.asahi-n...|\n",
      "|          200|  net-1-141.eden.com|\n",
      "+-------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def nasa_split_http(line):\n",
    "    parts = line.split(' ')\n",
    "    if (len(parts) > 2):\n",
    "        row = Row(\n",
    "            ip=parts[0],\n",
    "            http_response=parts[-2],\n",
    "        )\n",
    "        return row\n",
    "    else:\n",
    "        return {}\n",
    "#199.72.81.55 - - [01/Jul/1995:00:00:01 -0400] \"GET /history/apollo/ HTTP/1.0\" 200 6245\n",
    "#row = Row(\n",
    "#    ip=parts[0],\n",
    "#    date=parts[3]+' '+parts[4],\n",
    "#    http_method=parts[5].replace('\\\"', ''),\n",
    "#    http_url=parts[6],\n",
    "#    http_version=parts[7].replace('\"',''),\n",
    "#    http_response=parts[8],\n",
    "#    response_time=parts[9]\n",
    "#)\n",
    "\n",
    "nasa = sc.textFile(\"../data/nasa/NASA_access_log_Jul95\")\n",
    "nasa_rows = nasa.flatMap(lambda line: line.split('\\n')).map(nasa_split_http)\n",
    "# Convert to a pyspark.sql.DataFrame\n",
    "nasa_rows_df = nasa_rows.toDF()\n",
    "\n",
    "# Register the DataFrame for Spark SQL\n",
    "nasa_rows_df.registerTempTable(\"ip_response\")\n",
    "\n",
    "# Generate a new DataFrame with SQL using the SparkSession\n",
    "job_show = spark.sql(\"\"\"\n",
    "SELECT\n",
    "  *\n",
    "  FROM ip_response\n",
    "\"\"\")\n",
    "job_show.show()\n",
    "\n",
    "# Go back to an RDD\n",
    "#job_show.rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.3 Run an SQL query that outputs the number of occurrences of each HTTP response code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+\n",
      "|http_response|count(http_response)|\n",
      "+-------------+--------------------+\n",
      "|          200|             1701534|\n",
      "|         null|                   0|\n",
      "|          302|               46573|\n",
      "|          501|                  14|\n",
      "|          404|               10845|\n",
      "|          403|                  54|\n",
      "|          500|                  62|\n",
      "|          304|              132627|\n",
      "|          400|                   5|\n",
      "+-------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(http_response='200', count(http_response)=1701534),\n",
       " Row(http_response=None, count(http_response)=0),\n",
       " Row(http_response='302', count(http_response)=46573),\n",
       " Row(http_response='501', count(http_response)=14),\n",
       " Row(http_response='404', count(http_response)=10845),\n",
       " Row(http_response='403', count(http_response)=54),\n",
       " Row(http_response='500', count(http_response)=62),\n",
       " Row(http_response='304', count(http_response)=132627),\n",
       " Row(http_response='400', count(http_response)=5)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate a new DataFrame with SQL using the SparkSession\n",
    "job_show_53 = spark.sql(\"\"\"\n",
    "SELECT\n",
    "  http_response, count(http_response)\n",
    "  FROM ip_response\n",
    "  GROUP BY http_response\n",
    "\"\"\")\n",
    "job_show_53.show()\n",
    " \n",
    "# Go back to an RDD\n",
    "job_show_53.rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.4 Cachen Sie den Dataframe und führen Sie dieselbe Query nochmals aus! Messen Sie die Laufzeit für das Cachen und für die Ausführungszeit der Query!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache time:  0.002733469009399414\n",
      "+-------------+--------------------+\n",
      "|http_response|count(http_response)|\n",
      "+-------------+--------------------+\n",
      "|          200|             1701534|\n",
      "|         null|                   0|\n",
      "|          302|               46573|\n",
      "|          501|                  14|\n",
      "|          404|               10845|\n",
      "|          403|                  54|\n",
      "|          500|                  62|\n",
      "|          304|              132627|\n",
      "|          400|                   5|\n",
      "+-------------+--------------------+\n",
      "\n",
      "Query time:  25.073444604873657\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "nasa_rows_df.cache()\n",
    "\n",
    "start = time.time()\n",
    "nasa_rows_df.cache()\n",
    "end = time.time()\n",
    "print(\"Cache time: \",end - start)\n",
    "\n",
    "start = time.time()\n",
    "job_show_53 = spark.sql(\"\"\"\n",
    "SELECT\n",
    "  http_response, count(http_response)\n",
    "  FROM ip_response\n",
    "  GROUP BY http_response\n",
    "\"\"\")\n",
    "job_show_53.show()\n",
    "end = time.time()\n",
    "print(\"Query time: \",end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.5. Implement the same Query using the Dataframe API!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.6 Führen Sie diesselbe Query mit/ohne Cache und 8, 16 Cores aus! Dokumentieren und erklären Sie das Ergebnis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do not need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.7 Performance Analysis: \n",
    "* Create RDDs with 2x, 4x, 8x and 16x of the size of the NASA log dataset! Persist the dataset in the Spark Cache! Use an appropriate number of cores (e.g. 8 or 16)!\n",
    "* Measure and plot the response times for all datasets using a constant number of cores!\n",
    "* Plot the results!\n",
    "* Explain the results!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.8 Strong Scaling\n",
    "\n",
    "  * Measure the runtime for the query for 8, 16, 32, 64, 128, 256 cores for 1x and 16x datasets! Datasets cached in Memory!\n",
    "  * Compute the speedup and efficiency!\n",
    "  * Plot the responses!\n",
    "  * Explain the results!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.9 Convert the output to a Pandas dataframe and calculate the percentage of total for each response code!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
